{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyM/4NKXl4LSuDu4XMN4O/Pr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/superhj1987/agi-starter/blob/main/LLM_beauty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXNq4xyzP0lk"
      },
      "outputs": [],
      "source": [
        "#@title 初始化\n",
        "\n",
        "!pip install openai pandas transformers faiss-gpu\n",
        "\n",
        "%env OPENAI_API_KEY=xx\n",
        "%env HUGGINGFACE_API_KEY=xx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive',force_remount = True)\n",
        "root_dir = \"/content/drive/MyDrive/\"\n",
        "os.chdir(root_dir+\"/Colab Notebooks\")\n",
        "!pwd"
      ],
      "metadata": {
        "id": "EG8Cbz8qZ-Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ControlNet"
      ],
      "metadata": {
        "id": "ZIXFkU4INfV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install diffusers transformers xformers accelerate\n",
        "%pip install opencv-contrib-python\n",
        "%pip install controlnet_aux"
      ],
      "metadata": {
        "id": "plHAo-Y6NikH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 边缘检测\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers.utils import load_image\n",
        "from PIL import Image\n",
        "\n",
        "image_file = \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
        "original_image = load_image(image_file)\n",
        "\n",
        "def get_canny_image(original_image, low_threshold=100, high_threshold=200):\n",
        "  image = np.array(original_image)\n",
        "\n",
        "  image = cv2.Canny(image, low_threshold, high_threshold)\n",
        "  image = image[:, :, None]\n",
        "  image = np.concatenate([image, image, image], axis=2)\n",
        "  canny_image = Image.fromarray(image)\n",
        "  return canny_image\n",
        "\n",
        "canny_image = get_canny_image(original_image)\n",
        "\n",
        "def display_images(image1, image2):\n",
        "  # Combine the images horizontally\n",
        "  combined_image = Image.new('RGB', (image1.width + image2.width, max(image1.height, image2.height)))\n",
        "  combined_image.paste(image1, (0, 0))\n",
        "  combined_image.paste(image2, (image1.width, 0))\n",
        "  # Display the combined image\n",
        "  plt.imshow(combined_image)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "display_images(original_image, canny_image)\n"
      ],
      "metadata": {
        "id": "nSbzb_xfPiR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
        "import torch\n",
        "\n",
        "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# 在 GPU 显存不够用的时候，把不需要使用的模型从 GPU 显存里移除，放到内存里面\n",
        "pipe.enable_model_cpu_offload()\n",
        "\n",
        "# 通过我们安装好的 xformers 库来加速模型推理。\n",
        "pipe.enable_xformers_memory_efficient_attention()"
      ],
      "metadata": {
        "id": "44p7fs79QpjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \", best quality, extremely detailed\"\n",
        "prompt = [t + prompt for t in [\"Audrey Hepburn\", \"Elizabeth Taylor\", \"Scarlett Johansson\", \"Taylor Swift\"]]\n",
        "generator = [torch.Generator(device=\"cpu\").manual_seed(42) for i in range(len(prompt))]\n",
        "\n",
        "output = pipe(\n",
        "    prompt,\n",
        "    canny_image,\n",
        "    negative_prompt=[\"monochrome, lowres, bad anatomy, worst quality, low quality\"] * 4,\n",
        "    num_inference_steps=20,\n",
        "    generator=generator,\n",
        ")\n",
        "\n",
        "def draw_image_grids(images, rows, cols):\n",
        "  # Create a rows x cols grid for displaying the images\n",
        "  fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      axes[row, col].imshow(images[col + row * cols])\n",
        "  for ax in axes.flatten():\n",
        "      ax.axis('off')\n",
        "  # Display the grid\n",
        "  plt.show()\n",
        "\n",
        "draw_image_grids(output.images, 2, 2)"
      ],
      "metadata": {
        "id": "woFrKlMJQ7VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 动态捕捉\n",
        "\n",
        "from controlnet_aux import OpenposeDetector\n",
        "from diffusers.utils import load_image\n",
        "\n",
        "openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
        "\n",
        "image_file1 = \"./rodin.jpg\"\n",
        "original_image1 = load_image(image_file1)\n",
        "openpose_image1 = openpose(original_image1)\n",
        "\n",
        "image_file2 = \"./discobolos.jpg\"\n",
        "original_image2 = load_image(image_file2)\n",
        "openpose_image2 = openpose(original_image2)\n",
        "\n",
        "images = [original_image1, openpose_image1, original_image2, openpose_image2]\n",
        "draw_image_grids(images, 2, 2)"
      ],
      "metadata": {
        "id": "A0QEa1VRTRLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
        "from diffusers import UniPCMultistepScheduler\n",
        "import torch\n",
        "\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    \"fusing/stable-diffusion-v1-5-controlnet-openpose\", torch_dtype=torch.float16\n",
        ")\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    controlnet=controlnet,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_model_cpu_offload()\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "poses = [openpose_image1, openpose_image2, openpose_image1, openpose_image2]\n",
        "\n",
        "generator = [torch.Generator(device=\"cpu\").manual_seed(42) for i in range(4)]\n",
        "prompt1 = \"batman character, best quality, extremely detailed\"\n",
        "prompt2 = \"ironman character, best quality, extremely detailed\"\n",
        "\n",
        "output = pipe(\n",
        "    [prompt1, prompt1, prompt2, prompt2],\n",
        "    poses,\n",
        "    negative_prompt=[\"monochrome, lowres, bad anatomy, worst quality, low quality\"] * 4,\n",
        "    generator=generator,\n",
        "    num_inference_steps=20,\n",
        ")"
      ],
      "metadata": {
        "id": "e-uW0D7bVTaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Scribble\n",
        "\n",
        "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
        "from diffusers import UniPCMultistepScheduler\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers.utils import load_image\n",
        "from PIL import Image\n",
        "\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    \"lllyasviel/sd-controlnet-scribble\", torch_dtype=torch.float16\n",
        ")\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    controlnet=controlnet,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "pipe.enable_model_cpu_offload()\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "from diffusers.utils import load_image\n",
        "\n",
        "image_file = \"./img/scribble_dog.png\"\n",
        "scribble_image = load_image(image_file)\n",
        "\n",
        "generator = [torch.Generator(device=\"cpu\").manual_seed(2) for i in range(4)]\n",
        "prompt = \"dog\"\n",
        "prompt = [prompt + t for t in [\" in a room\", \" near the lake\", \" on the street\", \" in the forrest\"]]\n",
        "output = pipe(\n",
        "    prompt,\n",
        "    scribble_image,\n",
        "    negative_prompt=[\"lowres, bad anatomy, worst quality, low quality\"] * 4,\n",
        "    generator=generator,\n",
        "    num_inference_steps=50,\n",
        ")\n",
        "\n",
        "def draw_image_grids(images, rows, cols):\n",
        "  # Create a rows x cols grid for displaying the images\n",
        "  fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      axes[row, col].imshow(images[col + row * cols])\n",
        "  for ax in axes.flatten():\n",
        "      ax.axis('off')\n",
        "  # Display the grid\n",
        "  plt.show()\n",
        "\n",
        "draw_image_grids(output.images, 2, 2)"
      ],
      "metadata": {
        "id": "v4XwiAh9XP4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stable Diffusion"
      ],
      "metadata": {
        "id": "WJ4dz-f1WdJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers accelerate transformers"
      ],
      "metadata": {
        "id": "nP5y7kyoWcBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 文生图：DiffusionPipeline\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
        "pipeline.to(\"cuda\")\n",
        "image = pipeline(\"a photograph of an astronaut riding a horse\").images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "xsPtxhM1Wy1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SD步骤拆解\n",
        "\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
        "\n",
        "# 模型组件加载\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\")\n",
        "scheduler = PNDMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
        "\n",
        "torch_device = \"cuda\"\n",
        "vae.to(torch_device)\n",
        "text_encoder.to(torch_device)\n",
        "unet.to(torch_device)\n",
        "\n",
        "# 生成图片的参数初始化一下，包括文本、对应的图片分辨率，以及一系列模型中需要使用的超参数。\n",
        "\n",
        "import torch\n",
        "\n",
        "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
        "height = 512  # default height of Stable Diffusion\n",
        "width = 512  # default width of Stable Diffusion\n",
        "num_inference_steps = 25  # Number of denoising steps\n",
        "guidance_scale = 7.5  # Scale for classifier-free guidance\n",
        "generator = torch.manual_seed(42)  # Seed generator to create the inital latent noise\n",
        "batch_size = len(prompt)\n",
        "\n",
        "# 把对应的输入文本变成一个向量，然后再根据一个空字符串生成一个“无条件”的向量，最后把两个向量拼接在一起。我们实际生成图片的过程，就是逐渐从这个无条件的向量向输入文本表示的向量靠拢的过程。\n",
        "text_input = tokenizer(\n",
        "    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
        "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "\n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "#生成一系列随机噪声\n",
        "latents = torch.randn(\n",
        "    (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        ")\n",
        "latents = latents.to(torch_device)\n",
        "\n",
        "latents = latents * scheduler.init_noise_sigma\n",
        "\n",
        "# 显示 Generation 模块生成出来的图片信息，以及 Decoder 模块还原出来的最终图片。\n",
        "\n",
        "import PIL\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "def display_denoised_sample(sample, i):\n",
        "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "    image_processed = (image_processed + 1.0) * 127.5\n",
        "    image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "    display(f\"Denoised Sample @ Step {i}\")\n",
        "    display(image_pil)\n",
        "    return image_pil\n",
        "\n",
        "def display_decoded_image(latents, i):\n",
        "  # scale and decode the image latents with vae\n",
        "  latents = 1 / 0.18215 * latents\n",
        "  with torch.no_grad():\n",
        "    image = vae.decode(latents).sample\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "    images = (image * 255).round().astype(\"uint8\")\n",
        "    pil_images = [Image.fromarray(image) for image in images]\n",
        "    display(f\"Decoded Image @ step {i}\")\n",
        "    display(pil_images[0])\n",
        "    return pil_images[0]\n",
        "\n",
        "# 据前面指定的参数，循环了 25 步，每一步都通过 Scheduler 和 UNet 来进行图片去噪声的操作。并且每 5 步都把对应去噪后的图片信息，以及解码后还原的图片显示出来。\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "denoised_images = []\n",
        "decoded_images = []\n",
        "for i, t in enumerate(tqdm(scheduler.timesteps)):\n",
        "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "    latent_model_input = torch.cat([latents] * 2)\n",
        "\n",
        "    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
        "\n",
        "    # predict the noise residual\n",
        "    with torch.no_grad():\n",
        "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "    # perform guidance\n",
        "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "    # compute the previous noisy sample x_t -> x_t-1\n",
        "    latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
        "    if i % 5 == 0:\n",
        "      denoised_image = display_denoised_sample(latents, i)\n",
        "      decoded_image = display_decoded_image(latents, i)\n",
        "      denoised_images.append(denoised_image)\n",
        "      decoded_images.append(decoded_image)\n",
        "\n",
        "# Generation 生成的图像信息分辨率只有 64x64，而我们还原出来的图片分辨率是 512x512。\n",
        "\n",
        "print(latents.shape)\n",
        "latents = 1 / 0.18215 * latents\n",
        "with torch.no_grad():\n",
        "    image = vae.decode(latents).sample\n",
        "    print(image.shape)"
      ],
      "metadata": {
        "id": "X5vzEcPJ6BSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 图生图：StableDiffusionImg2ImgPipeline\n",
        "\n",
        "# !pip install transformers\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "\n",
        "device = \"cuda\"\n",
        "model_id_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "image_file = \"./00.jpg\"\n",
        "\n",
        "init_image = Image.open(image_file).convert(\"RGB\")\n",
        "init_image = init_image.resize((768, 512))\n",
        "\n",
        "prompt = \"ghibli style, a fantasy landscape with castles\"\n",
        "\n",
        "images = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images\n",
        "\n",
        "display(init_image)\n",
        "display(images[0])"
      ],
      "metadata": {
        "id": "sjShF7KO9LJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 提升分辨率：StableDiffusionUpscalePipeline\n",
        "\n",
        "from diffusers import StableDiffusionUpscalePipeline\n",
        "\n",
        "# load model and scheduler\n",
        "model_id = \"stabilityai/stable-diffusion-x4-upscaler\"\n",
        "pipeline = StableDiffusionUpscalePipeline.from_pretrained(\n",
        "    model_id, revision=\"fp16\", torch_dtype=torch.float16\n",
        ")\n",
        "pipeline = pipeline.to(\"cuda\")\n",
        "\n",
        "# let's download an  image\n",
        "low_res_img_file = \"/content/00.jpg\"\n",
        "low_res_img = Image.open(low_res_img_file).convert(\"RGB\")\n",
        "low_res_img = low_res_img.resize((128, 128))\n",
        "\n",
        "prompt = \"a white cat\"\n",
        "\n",
        "upscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\n",
        "\n",
        "low_res_img_resized = low_res_img.resize((512, 512))\n",
        "\n",
        "display(low_res_img_resized)\n",
        "display(upscaled_image)"
      ],
      "metadata": {
        "id": "yHez652cDmb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 使用社区模型\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "model_id = \"gsdf/Counterfeit-V2.5\"\n",
        "pipeline = DiffusionPipeline.from_pretrained(model_id)\n",
        "pipeline.to(\"cuda\")\n",
        "\n",
        "prompt = \"((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\"\n",
        "negative_prompt = \"EasyNegative, extra fingers,fewer fingers,\"\n",
        "image = pipeline(prompt=prompt, negative_prompt=negative_prompt).images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "0_EOrVVKELZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenClip"
      ],
      "metadata": {
        "id": "4TFAwwTkOCBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets matplotlib"
      ],
      "metadata": {
        "id": "p4XzIJazODSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 文本搜索图片\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from datasets import load_dataset\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from matplotlib import pyplot as plt\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "dataset = load_dataset(\"rajuptvs/ecommerce_products_clip\")\n",
        "\n",
        "training_split = dataset[\"train\"]\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "def get_image_features(image):\n",
        "    with torch.no_grad():\n",
        "        inputs = processor(images=[image], return_tensors=\"pt\", padding=True)\n",
        "        inputs.to(device)\n",
        "        features = model.get_image_features(**inputs)\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "def add_image_features(example):\n",
        "    example[\"features\"] = get_image_features(example[\"image\"])\n",
        "    return example\n",
        "\n",
        "# Apply the function to the training_split\n",
        "training_split = training_split.map(add_image_features)\n",
        "\n",
        "features = [example[\"features\"] for example in training_split]\n",
        "features_matrix = np.vstack(features)\n",
        "\n",
        "dimension = features_matrix.shape[1]\n",
        "\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(features_matrix.astype('float32'))\n",
        "\n",
        "def get_text_features(text):\n",
        "    with torch.no_grad():\n",
        "        inputs = processor(text=[text], return_tensors=\"pt\", padding=True)\n",
        "        inputs.to(device)\n",
        "        features = model.get_text_features(**inputs)\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "def search(query_text, top_k=5):\n",
        "    # Get the text feature vector for the input query\n",
        "    text_features = get_text_features(query_text)\n",
        "\n",
        "    # Perform a search using the FAISS index\n",
        "    distances, indices = index.search(text_features.astype(\"float32\"), top_k)\n",
        "\n",
        "    results = [\n",
        "        {\"image\": training_split[int(i)][\"image\"], \"distance\": distances[0][j]}\n",
        "        for j, i in enumerate(indices[0])\n",
        "    ]\n",
        "\n",
        "    return results\n",
        "\n",
        "query_text = \"A red dress\"\n",
        "results = search(query_text)\n",
        "\n",
        "# Display the search results\n",
        "def display_search_results(results):\n",
        "    fig, axes = plt.subplots(1, len(results), figsize=(15, 5))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for idx, result in enumerate(results):\n",
        "        axes[idx].imshow(result[\"image\"])\n",
        "        axes[idx].set_title(f\"Distance: {result['distance']:.2f}\")\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
        "    plt.show()\n",
        "\n",
        "display_search_results(results)"
      ],
      "metadata": {
        "id": "_57WSxGDK9R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace"
      ],
      "metadata": {
        "id": "kLJkuf74WUY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pipepline\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(task=\"sentiment-analysis\", device=0)\n",
        "preds = classifier(\"I am really happy today!\")\n",
        "print(preds)"
      ],
      "metadata": {
        "id": "MwvWsO_wW9NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Inference API\n",
        "\n",
        "import os, requests, json\n",
        "\n",
        "API_TOKEN = os.environ.get(\"HUGGINGFACE_API_KEY\")\n",
        "\n",
        "model = \"google/flan-t5-xxl\"\n",
        "API_URL = f\"https://api-inference.huggingface.co/models/{model}\"\n",
        "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\", \"Content-Type\": \"application/json\"}\n",
        "\n",
        "def query(payload, api_url=API_URL, headers=headers):\n",
        "    data = json.dumps(payload)\n",
        "    response = requests.request(\"POST\", api_url, headers=headers, data=data)\n",
        "    return json.loads(response.content.decode(\"utf-8\"))\n",
        "\n",
        "# question = \"Please answer the following question. What is the most famous city of China?\"\n",
        "# data = query({\"inputs\" : question})\n",
        "\n",
        "# print(data)\n",
        "\n",
        "model = \"hfl/chinese-pert-base\" #文本->向量\n",
        "API_URL = f\"https://api-inference.huggingface.co/models/{model}\"\n",
        "\n",
        "question = \"今天天气真不错！\"\n",
        "data = query({\"inputs\" : question}, api_url=API_URL)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "S2xdRNiDDL0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain"
      ],
      "metadata": {
        "id": "j4JTPPGs9uRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "FizqGvDoNFOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SimpleSequentialChain\n",
        "\n",
        "import openai, os\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain,SimpleSequentialChain\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", max_tokens=2048, temperature=0.5)\n",
        "en_to_zh_prompt = PromptTemplate( template=\"请把下面这句话翻译成英文： \\n\\n {question}?\", input_variables=[\"question\"] )\n",
        "question_prompt = PromptTemplate( template = \"{english_question}\", input_variables=[\"english_question\"] )\n",
        "zh_to_cn_prompt = PromptTemplate( input_variables=[\"english_answer\"], template=\"请把下面这一段翻译成中文： \\n\\n{english_answer}?\", )\n",
        "\n",
        "question_translate_chain = LLMChain(llm=llm, prompt=en_to_zh_prompt, output_key=\"english_question\")\n",
        "qa_chain = LLMChain(llm=llm, prompt=question_prompt, output_key=\"english_answer\")\n",
        "answer_translate_chain = LLMChain(llm=llm, prompt=zh_to_cn_prompt)\n",
        "\n",
        "chinese_qa_chain = SimpleSequentialChain(\n",
        "    chains=[question_translate_chain, qa_chain, answer_translate_chain], input_key=\"question\",\n",
        "    verbose=True)\n",
        "answer = chinese_qa_chain.run(question=\"请你作为一个机器学习的专家，介绍一下CNN的原理。\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "Go-NsGJ69Fda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LLMMathChain\n",
        "\n",
        "multiply_by_python_prompt = PromptTemplate(template=\"请写一段Python代码，计算{question}?\", input_variables=[\"question\"])\n",
        "math_chain = LLMChain(llm=llm, prompt=multiply_by_python_prompt, output_key=\"answer\")\n",
        "answer_code = math_chain.run({\"question\": \"352乘以493\"})\n",
        "\n",
        "from langchain.utilities import PythonREPL\n",
        "python_repl = PythonREPL()\n",
        "result = python_repl.run(answer_code)\n",
        "print(result)\n",
        "\n",
        "from langchain import LLMMathChain\n",
        "\n",
        "llm_math = LLMMathChain(llm=llm, verbose=True)\n",
        "result = llm_math.run(\"请计算一下352乘以493是多少?\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "iNxS7U_49Lz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LLMRequestChain\n",
        "\n",
        "from langchain.chains import LLMRequestsChain,TransformChain, SequentialChain\n",
        "\n",
        "template = \"\"\"在 >>> 和 <<< 直接是来自Google的原始搜索结果.\n",
        "请把对于问题 '{query}' 的答案从里面提取出来，如果里面没有相关信息的话就说 \"找不到\"\n",
        "请使用如下格式：\n",
        "Extracted:<answer or \"找不到\">\n",
        ">>> {requests_result} <<<\n",
        "Extracted:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    input_variables=[\"query\", \"requests_result\"],\n",
        "    template=template,\n",
        ")\n",
        "requests_chain = LLMRequestsChain(llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT))\n",
        "question = \"今天上海的天气怎么样？\"\n",
        "inputs = {\n",
        "    \"query\": question,\n",
        "    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\")\n",
        "}\n",
        "# result=requests_chain(inputs)\n",
        "# print(result)\n",
        "# print(result['output'])\n",
        "\n",
        "import re\n",
        "def parse_weather_info(weather_info: str) -> dict:\n",
        "    print(weather_info)\n",
        "    # 将天气信息拆分成不同部分\n",
        "    parts = weather_info.split('; ')\n",
        "\n",
        "    # 解析天气\n",
        "    weather = parts[0].strip()\n",
        "\n",
        "    # 解析温度范围，并提取最小和最大温度\n",
        "    temperature_range = parts[1].strip().replace('℃', '').split('~')\n",
        "    temperature_min = int(temperature_range[0])\n",
        "    temperature_max = int(temperature_range[1])\n",
        "\n",
        "    # 解析风向和风力\n",
        "    wind_parts = parts[2].split(' ')\n",
        "    wind_direction = wind_parts[0].strip()\n",
        "    wind_force = wind_parts[1].strip()\n",
        "\n",
        "    # 返回解析后的天气信息字典\n",
        "    weather_dict = {\n",
        "        'weather': weather,\n",
        "        'temperature_min': temperature_min,\n",
        "        'temperature_max': temperature_max,\n",
        "        'wind_direction': wind_direction,\n",
        "        'wind_force': wind_force\n",
        "    }\n",
        "\n",
        "    return weather_dict\n",
        "\n",
        "def transform_func(inputs: dict) -> dict:\n",
        "    text = inputs[\"output\"]\n",
        "    return {\"weather_info\" : parse_weather_info(text)}\n",
        "\n",
        "transformation_chain = TransformChain(input_variables=[\"output\"],\n",
        "                                      output_variables=[\"weather_info\"], transform=transform_func)\n",
        "\n",
        "final_chain = SequentialChain(chains=[requests_chain, transformation_chain],\n",
        "                              input_variables=[\"query\", \"url\"], output_variables=[\"weather_info\"])\n",
        "final_result = final_chain.run(inputs)\n",
        "print(final_result)"
      ],
      "metadata": {
        "id": "VEO5pSAxWMxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 多步提示语写测试"
      ],
      "metadata": {
        "id": "1GytMjBA94O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai, os\n",
        "\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAIChat\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = OpenAIChat(max_tokens=2048, temperature=0.5)\n",
        "multiple_choice = \"\"\"\n",
        "请针对 >>> 和 <<< 中间的用户问题，选择一个合适的工具去回答她的问题。只要用A、B、C的选项字母告诉我答案。\n",
        "如果你觉得都不合适，就选D。\n",
        "\n",
        ">>>{question}<<<\n",
        "\n",
        "我们有的工具包括：\n",
        "A. 一个能够查询商品信息，为用户进行商品导购的工具\n",
        "B. 一个能够查询订单信息，获得最新的订单情况的工具\n",
        "C. 一个能够搜索商家的退换货政策、运费、物流时长、支付渠道、覆盖国家的工具\n",
        "D. 都不合适\n",
        "\"\"\"\n",
        "multiple_choice_prompt = PromptTemplate(template=multiple_choice, input_variables=[\"question\"])\n",
        "choice_chain = LLMChain(llm=llm, prompt=multiple_choice_prompt, output_key=\"answer\")\n",
        "\n",
        "question = \"我想买一件衣服，但是不知道哪个款式好看，你能帮我推荐一下吗？\"\n",
        "print(choice_chain(question))\n",
        "\n",
        "question = \"我有一张订单，订单号是 2022ABCDE，一直没有收到，能麻烦帮我查一下吗？\"\n",
        "print(choice_chain(question))\n",
        "\n",
        "\n",
        "question = \"请问你们的货，能送到三亚吗？大概需要几天？\"\n",
        "print(choice_chain(question))\n",
        "\n",
        "question = \"今天天气怎么样？\"\n",
        "print(choice_chain(question))"
      ],
      "metadata": {
        "id": "x82H8UHQTaOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "def search_order(input: str) -> str:\n",
        "    return \"订单状态：已发货；发货日期：2023-01-01；预计送达时间：2023-01-10\"\n",
        "\n",
        "def recommend_product(input: str) -> str:\n",
        "    return \"红色连衣裙\"\n",
        "\n",
        "def faq(intput: str) -> str:\n",
        "    return \"7天无理由退货\"\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Search Order\",func=search_order,\n",
        "        description=\"useful for when you need to answer questions about customers orders\"\n",
        "    ),\n",
        "    Tool(name=\"Recommend Product\", func=recommend_product,\n",
        "         description=\"useful for when you need to answer questions about product recommendations\"\n",
        "    ),\n",
        "    Tool(name=\"FAQ\", func=faq,\n",
        "         description=\"useful for when you need to answer questions about shopping policies, like return policy, shipping policy, etc.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=False)\n",
        "\n",
        "question = \"我想买一件衣服，但是不知道哪个款式好看，你能帮我推荐一下吗？\"\n",
        "result = agent.run(question)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "GGxVZh_8VCEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"我有一张订单，订单号是 2022ABCDE，一直没有收到，能麻烦帮我查一下吗？\"\n",
        "result = agent.run(question)\n",
        "print(result)\n",
        "\n",
        "question = \"请问你们的货，能送到三亚吗？大概需要几天？\"\n",
        "result = agent.run(question)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "tPM-XUoCVar_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. 描述代码实现\n",
        "import openai, os\n",
        "import pandas as pd\n",
        "\n",
        "openai.api_key = openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "def gpt35(prompt, model=\"text-davinci-002\", temperature=0.4, max_tokens=1000,\n",
        "          top_p=1, stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"]):\n",
        "    response = openai.Completion.create(\n",
        "        model=model,\n",
        "        prompt = prompt,\n",
        "        temperature = temperature,\n",
        "        max_tokens = max_tokens,\n",
        "        top_p = top_p,\n",
        "        stop = stop\n",
        "        )\n",
        "    message = response[\"choices\"][0][\"text\"]\n",
        "    return message\n",
        "\n",
        "code = \"\"\"\n",
        "def format_time(seconds):\n",
        "    minutes, seconds = divmod(seconds, 60)\n",
        "    hours, minutes = divmod(minutes, 60)\n",
        "\n",
        "    if hours > 0:\n",
        "        return f\"{hours}h{minutes}min{seconds}s\"\n",
        "    elif minutes > 0:\n",
        "        return f\"{minutes}min{seconds}s\"\n",
        "    else:\n",
        "        return f\"{seconds}s\"\n",
        "\"\"\"\n",
        "\n",
        "def explain_code(function_to_test, unit_test_package=\"pytest\"):\n",
        "    prompt = f\"\"\"\"# How to write great unit tests with {unit_test_package}\n",
        "\n",
        "In this advanced tutorial for experts, we'll use Python 3.10 and `{unit_test_package}` to write a suite of unit tests to verify the behavior of the following function.\n",
        "```python\n",
        "{function_to_test}\n",
        "\n",
        "\n",
        "Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n",
        "- First,\"\"\"\n",
        "    response = gpt35(prompt)\n",
        "    return response, prompt\n",
        "\n",
        "code_explaination, prompt_to_explain_code = explain_code(code)\n",
        "print(code_explaination)"
      ],
      "metadata": {
        "id": "nfdP6qmmHeA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. 生成测试计划，数量不够时补充\n",
        "\n",
        "def generate_a_test_plan(full_code_explaination, unit_test_package=\"pytest\"):\n",
        "    prompt_to_explain_a_plan = f\"\"\"\n",
        "\n",
        "A good unit test suite should aim to:\n",
        "- Test the function's behavior for a wide range of possible inputs\n",
        "- Test edge cases that the author may not have foreseen\n",
        "- Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain\n",
        "- Be easy to read and understand, with clean code and descriptive names\n",
        "- Be deterministic, so that the tests always pass or fail in the same way\n",
        "\n",
        "`{unit_test_package}` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n",
        "\n",
        "For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n",
        "-\"\"\"\n",
        "    prompt = full_code_explaination + prompt_to_explain_a_plan\n",
        "    response = gpt35(prompt)\n",
        "    return response, prompt\n",
        "\n",
        "test_plan, prompt_to_get_test_plan = generate_a_test_plan(prompt_to_explain_code + code_explaination)\n",
        "print(test_plan)\n",
        "\n",
        "not_enough_test_plan = \"\"\"The function is called with a valid number of seconds\n",
        "    - `format_time(1)` should return `\"1s\"`\n",
        "    - `format_time(59)` should return `\"59s\"`\n",
        "    - `format_time(60)` should return `\"1min\"`\n",
        "\"\"\"\n",
        "\n",
        "approx_min_cases_to_cover = 7\n",
        "elaboration_needed = test_plan.count(\"\\n-\") +1 < approx_min_cases_to_cover\n",
        "if elaboration_needed:\n",
        "        prompt_to_elaborate_on_the_plan = f\"\"\"\n",
        "\n",
        "In addition to the scenarios above, we'll also want to make sure we don't forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets):\n",
        "-\"\"\"\n",
        "        more_test_plan, prompt_to_get_test_plan = generate_a_test_plan(prompt_to_explain_code + code_explaination + not_enough_test_plan + prompt_to_elaborate_on_the_plan)\n",
        "        print(more_test_plan)"
      ],
      "metadata": {
        "id": "UO9q_OnZXlL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. 生成测试用例\n",
        "\n",
        "def generate_test_cases(function_to_test, unit_test_package=\"pytest\"):\n",
        "    starter_comment = \"Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\"\n",
        "    prompt_to_generate_the_unit_test = f\"\"\"\n",
        "\n",
        "Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n",
        "```python\n",
        "import {unit_test_package}  # used for our unit tests\n",
        "\n",
        "{function_to_test}\n",
        "\n",
        "#{starter_comment}\"\"\"\n",
        "    full_unit_test_prompt = prompt_to_explain_code + code_explaination + test_plan + prompt_to_generate_the_unit_test\n",
        "    return gpt35(model=\"text-davinci-003\", prompt=full_unit_test_prompt, stop=\"```\"), prompt_to_generate_the_unit_test\n",
        "\n",
        "unit_test_response, prompt_to_generate_the_unit_test = generate_test_cases(code)\n",
        "print(unit_test_response)"
      ],
      "metadata": {
        "id": "DaeWaXV2ZhqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4. 语法校验\n",
        "\n",
        "import ast\n",
        "\n",
        "code_start_index = prompt_to_generate_the_unit_test.find(\"```python\\n\") + len(\"```python\\n\")\n",
        "code_output = prompt_to_generate_the_unit_test[code_start_index:] + unit_test_response\n",
        "print(code_output)\n",
        "try:\n",
        "    ast.parse(code_output)\n",
        "except SyntaxError as e:\n",
        "    print(f\"Syntax error in generated code: {e}\")"
      ],
      "metadata": {
        "id": "yhs1uinicA1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 语义检索"
      ],
      "metadata": {
        "id": "-a2-C7nB-1FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai, os\n",
        "import pandas as pd\n",
        "\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "COMPLETION_MODEL = \"text-davinci-003\"\n",
        "\n",
        "def generate_data_by_prompt(prompt):\n",
        "    response = openai.Completion.create(\n",
        "        engine=COMPLETION_MODEL,\n",
        "        prompt=prompt,\n",
        "        temperature=0.5,\n",
        "        max_tokens=2048,\n",
        "        top_p=1,\n",
        "    )\n",
        "    return response.choices[0].text\n",
        "\n",
        "prompt = \"\"\"请你生成50条淘宝网里的商品的标题，每条在30个字左右，品类是3C数码产品，标题里往往也会有一些促销类的信息，每行一条。\"\"\"\n",
        "data = generate_data_by_prompt(prompt)\n",
        "product_names = data.strip().split('\\n')\n",
        "df = pd.DataFrame({'product_name': product_names})\n",
        "df.product_name = df.product_name.apply(lambda x: x.split('.')[1].strip())\n",
        "df.head()\n",
        "\n",
        "clothes_prompt = \"\"\"请你生成50条淘宝网里的商品的标题，每条在30个字左右，品类是女性的服饰箱包等等，标题里往往也会有一些促销类的信息，每行一条。\"\"\"\n",
        "clothes_data = generate_data_by_prompt(clothes_prompt)\n",
        "clothes_product_names = clothes_data.strip().split('\\n')\n",
        "clothes_df = pd.DataFrame({'product_name': clothes_product_names})\n",
        "clothes_df.product_name = clothes_df.product_name.apply(lambda x: x.split('.')[1].strip())\n",
        "clothes_df.head()\n",
        "\n",
        "df = pd.concat([df, clothes_df], axis=0)\n",
        "df = df.reset_index(drop=True)\n",
        "display(df)\n",
        "\n",
        "from openai.embeddings_utils import get_embeddings\n",
        "import openai, os, backoff\n",
        "\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "embedding_model = \"text-embedding-ada-002\"\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
        "def get_embeddings_with_backoff(prompts, engine):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(prompts), batch_size):\n",
        "        batch = prompts[i:i+batch_size]\n",
        "        embeddings += get_embeddings(list_of_text=batch, engine=engine)\n",
        "    return embeddings\n",
        "\n",
        "prompts = df.product_name.tolist()\n",
        "prompt_batches = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]\n",
        "\n",
        "embeddings = []\n",
        "for batch in prompt_batches:\n",
        "    batch_embeddings = get_embeddings_with_backoff(prompts=batch, engine=embedding_model)\n",
        "    embeddings += batch_embeddings\n",
        "\n",
        "df[\"embedding\"] = embeddings\n",
        "df.to_parquet(\"data/taobao_product_title.parquet\", index=False)"
      ],
      "metadata": {
        "id": "vlwfIApKJ0jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 聊天对话"
      ],
      "metadata": {
        "id": "4gCdfFtH9_Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 用Gradio实现聊天对话\n",
        "\n",
        "!pip install gradio\n",
        "\n",
        "#coding-utf-8\n",
        "import gradio as gr\n",
        "import openai\n",
        "import os\n",
        "\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "class Conversation:\n",
        "    def __init__(self, prompt, num_of_round):\n",
        "        self.prompt = prompt\n",
        "        self.num_of_round = num_of_round\n",
        "        self.messages = []\n",
        "        self.messages.append({\"role\": \"system\", \"content\": self.prompt})\n",
        "\n",
        "    def ask(self, question):\n",
        "        try:\n",
        "            self.messages.append({\"role\": \"user\", \"content\": question})\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=self.messages,\n",
        "                temperature=0.5,\n",
        "                max_tokens=2048,\n",
        "                top_p=1,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return e\n",
        "\n",
        "        message = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": message})\n",
        "\n",
        "        if len(self.messages) > self.num_of_round*2 + 1:\n",
        "            del self.messages[1:3]\n",
        "        return message\n",
        "\n",
        "prompt = \"\"\"你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:\n",
        "1. 你的回答必须是中文\n",
        "2. 回答限制在100个字以内\"\"\"\n",
        "\n",
        "conv = Conversation(prompt, 10)\n",
        "\n",
        "def answer(question, history=[]):\n",
        "    history.append(question)\n",
        "    response = conv.ask(question)\n",
        "    history.append(response)\n",
        "    responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
        "    return responses, history\n",
        "\n",
        "with gr.Blocks(css=\"#chatbot{height:300px} .overflow-y-auto{height:500px}\") as demo:\n",
        "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
        "    state = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
        "\n",
        "    txt.submit(answer, [txt, state], [chatbot, state])\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "FOOrDbIiOXf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 返回token消耗数量\n",
        "\n",
        "import openai\n",
        "import os\n",
        "\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "class Conversation2:\n",
        "    def __init__(self, prompt, num_of_round):\n",
        "        self.prompt = prompt\n",
        "        self.num_of_round = num_of_round\n",
        "        self.messages = []\n",
        "        self.messages.append({\"role\": \"system\", \"content\": self.prompt})\n",
        "\n",
        "    def ask(self, question):\n",
        "        try:\n",
        "            self.messages.append( {\"role\": \"user\", \"content\": question})\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=self.messages,\n",
        "                temperature=0.5,\n",
        "                max_tokens=2048,\n",
        "                top_p=1,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return e\n",
        "\n",
        "        message = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        num_of_tokens = response['usage']['total_tokens']\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": message})\n",
        "\n",
        "        if len(self.messages) > self.num_of_round*2 + 1:\n",
        "            del self.messages[1:3]\n",
        "        return message, num_of_tokens\n",
        "\n",
        "conv2 = Conversation2(prompt, 3)\n",
        "questions = [question1, question2, question3, question4]\n",
        "for question in questions:\n",
        "    answer, num_of_tokens = conv2.ask(question)\n",
        "    print(\"询问 {%s} 消耗的token数量是 : %d\" % (question, num_of_tokens))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sRwmDCGlNs9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title OpenAI Chat接口\n",
        "\n",
        "import openai\n",
        "import os\n",
        "\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "class Conversation:\n",
        "    def __init__(self, prompt, num_of_round):\n",
        "        self.prompt = prompt\n",
        "        self.num_of_round = num_of_round\n",
        "        self.messages = []\n",
        "        self.messages.append({\"role\": \"system\", \"content\": self.prompt})\n",
        "\n",
        "    def ask(self, question):\n",
        "        try:\n",
        "            self.messages.append({\"role\": \"user\", \"content\": question})\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=self.messages,\n",
        "                temperature=0.5,\n",
        "                max_tokens=2048,\n",
        "                top_p=1,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return e\n",
        "\n",
        "        message = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": message})\n",
        "\n",
        "        if len(self.messages) > self.num_of_round*2 + 1:\n",
        "            del self.messages[1:3]\n",
        "        return message\n",
        "\n",
        "prompt = \"\"\"你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:\n",
        "1. 你的回答必须是中文\n",
        "2. 回答限制在100个字以内\"\"\"\n",
        "conv1 = Conversation(prompt, 2)\n",
        "question1 = \"你是谁？\"\n",
        "print(\"User : %s\" % question1)\n",
        "print(\"Assistant : %s\\n\" % conv1.ask(question1))\n",
        "\n",
        "question2 = \"请问鱼香肉丝怎么做？\"\n",
        "print(\"User : %s\" % question2)\n",
        "print(\"Assistant : %s\\n\" % conv1.ask(question2))\n",
        "\n",
        "question3 = \"那蚝油牛肉呢？\"\n",
        "print(\"User : %s\" % question3)\n",
        "print(\"Assistant : %s\\n\" % conv1.ask(question3))\n",
        "\n",
        "question4 = \"我问你的第一个问题是什么？\"\n",
        "print(\"User : %s\" % question4)\n",
        "print(\"Assistant : %s\\n\" % conv1.ask(question4))"
      ],
      "metadata": {
        "id": "lvIXnFj4G8RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 基础使用"
      ],
      "metadata": {
        "id": "GU8nC9Xh-RG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title OpenAI Embedding接口\n",
        "\n",
        "from openai.embeddings_utils import get_embedding\n",
        "\n",
        "text = \"让我们来算算Embedding\"\n",
        "\n",
        "embedding_ada = get_embedding(text, engine=\"text-similarity-davinci-001\")\n",
        "print(\"embedding-ada: \", len(embedding_ada))"
      ],
      "metadata": {
        "id": "8HHbv8zsuUrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 基于Completion接口的聊天机器人\n",
        "\n",
        "import openai\n",
        "import os\n",
        "\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "def ask_gpt3(prompt):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=512,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.5,\n",
        "    )\n",
        "\n",
        "    message = response.choices[0].text.strip()\n",
        "    return message\n",
        "\n",
        "print(\"你好，我是一个聊天机器人，请你提出你的问题吧?\")\n",
        "\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "def generate_prompt(prompt, questions, answers):\n",
        "    num = len(answers)\n",
        "    for i in range(num):\n",
        "        prompt += \"\\n Q : \" + questions[i]\n",
        "        prompt += \"\\n A : \" + answers[i]\n",
        "    prompt += \"\\n Q : \" + questions[num] + \"\\n A : \"\n",
        "    return prompt\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"> \")\n",
        "    questions.append(user_input)\n",
        "    if user_input.lower() in [\"bye\", \"goodbye\", \"exit\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    prompt = generate_prompt(\"\", questions, answers)\n",
        "\n",
        "    answer = ask_gpt3(prompt)\n",
        "    print(answer)\n",
        "    answers.append(answer)"
      ],
      "metadata": {
        "id": "J4XvbF3XTfQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title OpenAI Completion接口\n",
        "\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "COMPLETION_MODEL = \"text-davinci-003\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "Consideration proudct : 工厂现货PVC充气青蛙夜市地摊热卖充气玩具发光蛙儿童水上玩具\n",
        "\n",
        "1. Compose human readable product title used on Amazon in english within 20 words.\n",
        "2. Write 5 selling points for the products in Amazon.\n",
        "3. Evaluate a price range for this product in U.S.\n",
        "\n",
        "Output the result in json format with three properties called title, selling_points and price_range\n",
        "\"\"\"\n",
        "\n",
        "def get_response(prompt):\n",
        "    completions = openai.Completion.create (\n",
        "        engine=COMPLETION_MODEL,\n",
        "        prompt=prompt,\n",
        "        max_tokens=512,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.0,\n",
        "    )\n",
        "    message = completions.choices[0].text\n",
        "    return message\n",
        "\n",
        "print(get_response(prompt))"
      ],
      "metadata": {
        "id": "QHrdFzreQYCT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}